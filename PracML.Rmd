---
title: "Activity tracking analysis and prediction"
author: "Mandar Joshi"
date: "31/7/2019"
output: 
  html_document:
    keep_md: yes
--- 

## Overview

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. The objective of the project is to evaluate the correctness of the weightligting execises of the participants.

Following are the steps to be followed:

1. Data cleaning and Preprocessing
2. Exploratory Data Analysis
3. Prediction Model Selection
4. Predicting Test Set Output

## Data Preprocessing 

load the training and testing set (already segregated and provided) from the online sources  and then split the training set further into training and test sets. 


```{r echo=TRUE}
library(caret)

traintotal <- read.csv("pml-training.csv")
test_set <- read.csv("pml-testing.csv")

set.seed(123)

labels <- createDataPartition(traintotal$classe, p = 0.8, list = FALSE)
train_set <- traintotal[labels, ]
cross_val <- traintotal[-labels, ]

dim(train_set)
dim(cross_val)
```

We see a lot of 'na' present in the dataset. Thus, we first clean the data by removing na where the percentage of 'na' is greater than 30%. Also, there are a lot variables with a variance almost equal to zero. We remove these too.


```{r echo=TRUE}

labels_na <- apply(train_set, 2, function(x) mean(is.na(x))) > 0.5
train_set <- train_set[, -which(labels_na, labels_na == FALSE)]
cross_val <- cross_val[, -which(labels_na, labels_na == FALSE)]
test_set  <- test_set[, -which(labels_na, labels_na == FALSE)]

train_set <- train_set[ , -(1:5)]
cross_val <- cross_val[ , -(1:5)]
test_set  <- test_set[ , -(1:5)]

nearZero_index <- nearZeroVar(train_set)
train_set <- train_set[ ,-nearZero_index]
cross_val <- cross_val[ ,-nearZero_index]
test_set <- test_set[ ,-nearZero_index]

dim(train_set)
dim(cross_val)
dim(test_set)
```

## Exploratory Data analysis:

Now we a correlation analysis on the training dataset. The dark marked squares (either blue or brown) indicate high correlation (positive or negative).

``` {r fig.width=9, fig.height=9}
library(corrplot)
corrMat <- cor(train_set[,-54])
corrplot(corrMat, method = "square", type = "upper", tl.cex = 0.8, tl.col = rgb(0,0,0))
```

We further run a principal component analysis (PCA) on to the training set.

``` {r}
pca_mod = prcomp(train_set[,-54], center = TRUE, scale. = TRUE)

var_pca <- pca_mod$sdev^2
per_var <- (var_pca / sum(var_pca))*100

cov_num <- 35
sum(per_var[1:cov_num])
```

Thus, after performing PCA we cover almost 95% of the data variance in first 25 components.

```{r echo=TRUE}
plot(c(1:length(per_var)), per_var, type = "l", xlab = "principal components",
     ylab = "% of variance", main = "Cumulative % coverage of data variance", col = "grey")
points(c(1:length(per_var)), per_var, pch = 20)
abline(v = cov_num, col = "green", lty = 2, lwd = 2)
legend("topright",legend=c(" > 95% variance coverage"),
       col=c("green"),lty = 2, lwd = 2)

library(ggplot2)
pca_plot <- qplot(pca_mod$x[,1],pca_mod$x[,2], color = factor(train_set$classe), xlab = "Principal component 1", ylab = "Principal component 2")

pca_plot + scale_color_manual(values=c("red","magenta","green","cyan","blue"))

#library("plot3D")
#scatter3D(pca_mod$x[,1], pca_mod$x[,2],pca_mod$x[,3], col.var = as.integer(train_set$classe), col =c("red","magenta","green","cyan","blue"), bty = "g", phi = -10, theta = 90, pch = 20, cex = 0.5, xlab = "PC - 1", ylab = "PC - 2", zlab = "PC - 3")

train_with_pca <- data.frame(classe = train_set$classe, pca_mod$x)
train_with_pca <- train_with_pca[,1:(cov_num+1)]

test_data <- predict(pca_mod, newdata = cross_val)
test_data <- as.data.frame(test_data)
test_data <- test_data[,1:cov_num]
```

## Prediction Model(s)

### Decision Tree:

First we will build a decision tree model using the training data (w/o tranformed using PCA).

```{r}
library(rpart)
set.seed(123)

model_DecTree <- rpart(classe ~ ., data = train_set, method = "class")

predict_DecTree <- predict(model_DecTree, cross_val, type = "class")

confMat_DecTree <- confusionMatrix(predict_DecTree, cross_val$classe)
confMat_DecTree
```

### Random Forest (with PCA tranformed features)

``` {r}
set.seed(123)
control <- trainControl(method = "cv", number = 3, verboseIter=FALSE)
model_RF <- train(classe ~ ., data = train_with_pca, method = "rf", trControl = control)

predict_RF <- predict(model_RF, test_data)

confMat_RF <- confusionMatrix(predict_RF, cross_val$classe)
confMat_RF
```

## Predictions for the test set:

As we see Random forest provides better results for the cross-validation set, we select it for test set prediction.

```{r}

test_data <- predict(pca_mod, newdata = test_set)
test_data <- as.data.frame(test_data)
test_data <- test_data[,1:cov_num]

predict_RF <- predict(model_RF, test_data)
predict_RF

```
## Acknowledgements

data source : []{http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har}
